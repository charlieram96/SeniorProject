import scrapy
from scrapy.crawler import CrawlerProcess
from ws_seniorproject.items import WsSeniorprojectItem

class TestfileSpider(scrapy.Spider):
    
    # start section 1:
    # user search input
    # this allows users to input the items they would like to search for (i.e shoes, pants, tv's).
    # Amazon's url for a search looks like this: https://www.amazon.com/s?k= 
    # if we want to search for shoes, we add 'shoes' at the end of this usrl (https://www.amazon.com/s?k=shoes)
    # if the search is more than one word (i.e 'nike shoes', 'khaki pants'), the url requires a '+' between words
    # example: user input = nike shoes 
    # url = https://www.amazon.com/s?k=nike+shoes
    # the following code will generate such url and input it into 'start_urls'

    name = 'testfile'
    allowed_domains = ['amazon.com']
    search_terms = input("Enter search term: ")
    print("Search term: " + search_terms)
    
    search_terms_list = search_terms.split()
    
    if len(search_terms_list) == 1:
        url_to_crawl = 'https://www.amazon.com/s?k=' + search_terms
    else:
        url_to_crawl = 'https://www.amazon.com/s?k=' + search_terms_list[0]
        for remaining_terms in search_terms_list[1:]:
            url_to_crawl = url_to_crawl + "+" + remaining_terms

    
    
    # start_urls is the list of urls scrapy automatically looks for to crawl when the program is run. We are only
    # feeding it one url: the one generated by our code above. 

    start_urls = [url_to_crawl]

    # end section 1

    # start section 2
    # the follwowing function is the default parsing function called by scrapy to crawl the urls in 'start_urls'
    # the function tells scrapy what to scrape from the url. For this project, multiple urls have to be crawled:

    def parse(self, response):
        
        # the follwing line of code will look for all product urls after we search for a product. Amazon lists about 50-60
        # products for every search. However, as of now we are asking scrapy to extract the first 9 product urls [0:9]
        # and store them in a list named 'urls'

        # bug that was fixed: the code generated a url and an unwanted duplicate of the url. This was fixed by combining
        # css and xpath selectors to look for an h2 tag with the classes a-size-mini, a-spacing-none, and s-line-clamp-2
        # the xpath selector would then look for the href value and return the url. The duplicate urls came from not including
        # the classes in the h2 tag above the href value.

        urls = response.css('h2.a-size-mini.a-spacing-none.a-color-base.s-line-clamp-2 a').xpath('@href')[0:10].extract()
        
        # each url from the list 'urls' must individually be crawled. To allow for this, we run a for-loop that will
        # send a request to crawl a new url. For each url crawl request, we call the function 'parse_link'. 
        # parse_link will tell scrapy what we want to scrape from each url containing our individual products.  
        
        for product in urls:
            product_url = "https://www.amazon.com" + product
            request = scrapy.Request(product_url, callback=self.parse_link)
            yield request    

    # end section 2

    # start section 3
    # the following function 'parse_link' is called for every product. It currently scrapes the following fields:
    # 
    # product name
    # product price
    # product rating (out of 5 stars)
    # number of ratings
    # percent of ratings that are 5 stars
    # percent of ratings that are 4 stars
    # percent of ratings that are 3 stars
    # percent of ratings that are 2 stars
    # percent of ratings that are 1 stars

    # bugs that were fixed:
    # product name returned multiple spaces and '\n' fields along with the text for the product name. This was fixed by
    # simply calling the .strip() function on the extracted result. To allow the .strip() function to be added, 'title'
    # had to return a string instead of a list, which was made possible by adding '[0]' to the code, returning the first
    # value in the list as a string. This was convenient as all the other values following product name 
    # also returned strings.

    # percents of 1 - 5 stars was not returning the right values. This was fixed by combining css and xpath selectors
    # to allow the 'aria-label' to be selected. Additionally, the 6th - 10th values in the list returned the values we 
    # needed, which is why these are called in order: 5, 6, 7, 8, and 9.

    def parse_link(self, response):
        items = WsSeniorprojectItem()
        try:
            title = response.css('span.a-size-large#productTitle::text')[0].extract().strip(' \n')
        except IndexError:
            title = ''

        try:
            price = response.css('span.a-size-medium.a-color-price.priceBlockBuyingPriceString#priceblock_ourprice::text')[0].extract()
        except IndexError:
            price = ''

        try:
            rating = response.css('span.a-icon-alt::text')[0].extract()
        except IndexError:
            rating = ''

        try:
            rating_count = response.css('span.a-size-base#acrCustomerReviewText::text')[0].extract()
        except IndexError:
            rating_count = ''

        #percent_5_star = response.css('div.a-meter').xpath('@aria-label')[5].extract()
        try:
            percent_5_star = response.css('td.a-text-right span.a-size-base a.a-link-normal::text')[0].extract().strip(' \n')
        except IndexError:
            percent_5_star = ''

        #percent_4_star = response.css('div.a-meter').xpath('@aria-label')[6].extract()
        try: 
            percent_4_star = response.css('td.a-text-right span.a-size-base a.a-link-normal::text')[1].extract().strip(' \n')
        except IndexError:
            percent_4_star = ''

        #percent_3_star = response.css('div.a-meter').xpath('@aria-label')[7].extract()
        try: 
            percent_3_star = response.css('td.a-text-right span.a-size-base a.a-link-normal::text')[2].extract().strip(' \n')
        except IndexError:
            percent_3_star = ''

        #percent_2_star = response.css('div.a-meter').xpath('@aria-label')[8].extract()
        try: 
            percent_2_star = response.css('td.a-text-right span.a-size-base a.a-link-normal::text')[3].extract().strip(' \n')
        except IndexError:
            percent_2_star = ''

        #percent_1_star = response.css('div.a-meter').xpath('@aria-label')[9].extract()
        try:
            percent_1_star = response.css('td.a-text-right span.a-size-base a.a-link-normal::text')[4].extract().strip(' \n')
        except IndexError:
            percent_1_star = ''
        
        try:
            category = response.css('span.zg_hrsr_ladder a::text').extract()
        except IndexError:
            category = ''

        try:
            rank = response.css('#SalesRank::text')[1].extract().strip(' \n (')
        except IndexError:
            rank = ''

        try:
            answered = response.css('a#askATFLink span.a-size-base::text')[0].extract().strip(' \n')
        except IndexError:
            answered = ''

        try:
            prod_desc = response.css('div#productDescription p::text')[0].extract().strip(' \n')
        except IndexError:
            prod_desc = 'Product description unavailable'

        try:
            descriptMain1 = response.css("ul.a-unordered-list.a-vertical.a-spacing-none span.a-list-item::text").extract()
        except IndexError:
            descriptMain1 = ''

        try:
            descriptMain = descriptMain1[0].strip(' \n\t')
        except IndexError:
            descriptMain = ''

        if len(descriptMain1) > 1:
            for item in descriptMain1[1:]:
                if item != None:
                    descriptMain = descriptMain + ", " + item.strip(' \n\t')

        try:
            if category != None:
                if len(category) == 1:
                    category_final = category
                else:
                    category_final = category[0]
                    for categories in category[1:]:
                        category_final = category_final + ", " + categories
            else:
                category_final = ''
        except IndexError:
            category_final = ''

        try: 
            dim = response.css('ul.a-unordered-list.a-nostyle.a-vertical.a-spacing-none li span.a-list-item span::text')[1].extract()
        except IndexError:
            dim = 'Dimensions unavailable'

        try: 
            asin = response.css('ul.a-unordered-list.a-nostyle.a-vertical.a-spacing-none li span.a-list-item span::text')[6].extract()
        except IndexError:
            asin = 'ASIN number unavailable'

        try:
            fit = response.css('div.a-section a#HIF_link::text')[0].extract().strip('FitsAasexpected ()')
            if fit == '\n':
                fit2 = response.css('div.a-section a#HIF_link span.a-size-base::text')[0].extract().strip('Fits as expected ()')
                items['Fit_As_Expected'] = fit2
            else:
                items['Fit_As_Expected'] = fit
        except IndexError:
            fit = 'Fit not available'
            items['Fit_As_Expected'] = fit

        if title != None:
            items['Product'] = title
        else:
            items['Product'] = []
        
        if price != None:
            items['Price'] = price
        else:
            items['Price'] = []
        
        if rating != None:
            items['Rating'] = rating
        else:
            items['Rating'] = []
        
        if rating_count != None:
            items['Rating_Count'] = rating_count
        else:
            items['Rating_Count'] = []
        
        if percent_5_star != None:
            items['Five_Stars'] = percent_5_star
        else:
            items['Five_Stars'] = []
        
        if percent_4_star != None:
            items['Four_Stars'] = percent_4_star
        else: 
            items['Four_Stars'] = []
        
        if percent_3_star != None:
            items['Three_Stars'] = percent_3_star
        else: 
            items['Three_Stars'] = []
        
        if percent_2_star != None:
            items['Two_Stars'] = percent_2_star
        else:
            items['Two_Stars'] = []

        if percent_1_star != None:
            items['One_Star'] = percent_1_star
        else:
            items['One_Star'] = []

        items['Category'] = category_final
        
        if rank != None:
            items['Sales_Rank'] = rank
        else: 
            items['Sales_Rank'] = []
        
        items['Answered_Questions'] = answered
        items['Description_Main'] = descriptMain
        items['Description_Product'] = prod_desc
        items['Dimensions'] = dim
        items['ASIN_Number'] = asin
        #items['Prime']
        #items['First_Listed']

        yield items

    # end section 3

#process = CrawlerProcess({'USER_AGENT': 'Mozilla/5.0', 'FEED_FORMAT': 'json', 'FEED_URI': 'data.json'})
#process.crawl(TestfileSpider)
#process.start()

